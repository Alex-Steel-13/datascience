{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_a_Network.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex-Steel-13/datascience/blob/main/Fashion_Mnist_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjX6Fjgt0n93"
      },
      "source": [
        "###DO NOT EDIT THIS CODE\n",
        "################################################################################################################################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet50\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torchvision.datasets as datasets # Has standard datasets we can import in a nice way\n",
        "import torchvision.transforms as transforms # Transformations we can perform on our dataset\n",
        "import torch.nn.functional as F # All functions that don't have any parameters\n",
        "from torch.utils.data import DataLoader, Dataset # Gives easier dataset managment and creates mini batches\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
        "from PIL import Image\n",
        "\n",
        "# GPUs are 3x faster than CPU. Better to use if it is available \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define Loss Function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# This function returns the number of parameters in the model\n",
        "def num_params(model):\n",
        "  return sum([p.numel() for p in model.parameters()])\n",
        "\n",
        "# Define a Training Function. This function will: compute the forward pass, backpropagate,\n",
        "# update the weights, and repeat the steps for a given number of epochs. At each epoch, \n",
        "# it will output the training loss and test loss at every step\n",
        "def train(epochs, model, trainloader, testloader, optimizer, loss_function):\n",
        "  for epoch in range(epochs):\n",
        "    loss_epoch = np.array([])\n",
        "    train_correct, train_total = 0, 0\n",
        "    test_correct, test_total = 0, 0\n",
        "\n",
        "    for data, labels in trainloader:\n",
        "      # convert into GPU objects if needed\n",
        "      input_data = data.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      predict = model(input_data)\n",
        "      \n",
        "      # backward pass\n",
        "      loss = loss_function(predict, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # update parameters (weights and biases)\n",
        "      optimizer.step()\n",
        "\n",
        "      # store progress\n",
        "      loss_epoch = np.append(loss_epoch, loss.item())\n",
        "\n",
        "    # evaluate test accuracy\n",
        "    for data, labels in testloader:\n",
        "      input_data = data.to(device)\n",
        "      labels = labels.to(device)\n",
        "      predict = model(input_data)\n",
        "      for i, out in enumerate(predict):\n",
        "        pred = torch.argmax(out)\n",
        "        if pred == labels[i]:\n",
        "          test_correct+=1\n",
        "        test_total+=1\n",
        "\n",
        "    test_accuracy = test_correct/test_total    \n",
        "  \n",
        "    print('epoch [{}/{}], training loss:{:.4f}, test accuracy:{:.4f}'.format(epoch+1, epochs, np.mean(loss_epoch), test_accuracy))\n",
        "################################################################################################################################"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbeRdw4N1ZfM"
      },
      "source": [
        "# **Load Dataset**\n",
        "\n",
        "Available datasets are: MNIST, CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iVrE2cdmVXY"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWbgwSn41Fyy"
      },
      "source": [
        "# download and load data\n",
        "batch_size = 512\n",
        "\n",
        "# download and transform train dataset\n",
        "train_loader = torch.utils.data.DataLoader(datasets.FashionMNIST('./mnist_data', download=True, train=True, transform=transforms.Compose([\n",
        "                                                transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
        "                                                transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
        "                                                ])), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# download and transform test dataset\n",
        "test_loader = torch.utils.data.DataLoader(datasets.FashionMNIST('./mnist_data', download=True, train=False, transform=transforms.Compose([\n",
        "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
        "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
        "                                                          ])), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it is a good idea to take a look at the data. Here we see it is a 28x28 grayscale image\n",
        "for data, labels in train_loader:\n",
        "  print(data[0].size())\n",
        "  break"
      ],
      "metadata": {
        "id": "nn-luYwzxz4O",
        "outputId": "d617e8b1-c0ce-43d2-a128-9879f4d1ad30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTE0ttEd2CX9"
      },
      "source": [
        "# **Build a Network and Define Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8UHuwEOfO0s"
      },
      "source": [
        "########################\n",
        "####                #### \n",
        "#### EDIT THIS CELL ####\n",
        "####                ####\n",
        "########################\n",
        "\n",
        "\n",
        "learning_rate = 10e-3\n",
        "weight_decay = 10e-5\n",
        "n_epochs = 10\n",
        "\n",
        "# neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    ### Define Layers\n",
        "    self.ll1 = nn.Linear(784,512)\n",
        "    self.ll2 = nn.Linear(512,128)\n",
        "    self.ll3 = nn.Linear(128,64)\n",
        "    self.ll4 = nn.Linear(64,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.ll1(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    x = self.ll2(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    x = self.ll3(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    x = self.ll4(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    return x\n",
        "  \n",
        "# Every time you edit the neural network, you'll have to update this cell\n",
        "# Create model object\n",
        "model = NeuralNetwork().to(device)\n",
        "\n",
        "# Loads Adam optimizer, which implements a version of gradient descent\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the structure of your network\n",
        "print(model)\n",
        "\n",
        "# apply your model to a single input. This helps check that \n",
        "# the dimensions are correct\n",
        "model(torch.rand(1,1,28,28, device=device))"
      ],
      "metadata": {
        "id": "eccjx7zG0Mbk",
        "outputId": "7cc8f807-d495-4f21-8852-e05dd8b1889d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (ll1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (ll2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (ll3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (ll4): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4883, 0.4067, 0.4613, 0.5217, 0.5284, 0.4916, 0.5036, 0.3672, 0.5995,\n",
              "         0.6410]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6vNthxXAbHg"
      },
      "source": [
        "# **Train and Validate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgFfMHXeAgca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb698c1-83c2-4e74-e2e2-2ea2ef43f560"
      },
      "source": [
        "train(n_epochs, model, train_loader, test_loader, optimizer, loss_function)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [1/10], training loss:1.8431, test accuracy:0.2527\n",
            "epoch [2/10], training loss:1.7399, test accuracy:0.3255\n",
            "epoch [3/10], training loss:1.7018, test accuracy:0.4731\n",
            "epoch [4/10], training loss:1.6687, test accuracy:0.4726\n",
            "epoch [5/10], training loss:1.6607, test accuracy:0.4253\n",
            "epoch [6/10], training loss:1.6557, test accuracy:0.4635\n",
            "epoch [7/10], training loss:1.6475, test accuracy:0.4843\n",
            "epoch [8/10], training loss:1.6353, test accuracy:0.6213\n",
            "epoch [9/10], training loss:1.6152, test accuracy:0.6609\n",
            "epoch [10/10], training loss:1.6074, test accuracy:0.7282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "llQ_ONSbG6_P"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}